%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size
\usepackage[utf8]
{inputenc}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[hidelinks]{hyperref}
\usepackage{natbib} % bibliography
\usepackage{float}
\usepackage{graphicx}
\DeclareGraphicsExtensions{.png,.jpg,.pdf}

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands

\usepackage{titlesec}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Australian National University} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge A MC-AIXI-CTW Implementation\\ Group Project \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Johannes Kirschner\\ Kerry Olesen\\ Jesse Wu} % Your name

\date{\normalsize\today} % Today's date or a custom date


%\titleformat{\section}{\large\centering\normalfont\scshape}{}{0em}{}
%\titleformat{\subsection}{\centering\normalfont}{\roman{subsection})}{0em}{}

  

\begin{document}

\maketitle % Print the title

\section{Introduction}
The AIXI model \cite{Hutter:04uaibook} is an attempt to solve the general AI problem. The AIXI agent interacts with an environment in cycles. Denote by $\mathcal{A}, \mathcal{O}$ and $\mathcal{R}$ an action, observation and reward space respectively. In each cycle, AIXI takes an action $a \in \mathcal{A}$ and receives an observation $o \in \mathcal{O}$ and a reward $r \in \mathcal{R}$. The goal of the agent is to maximize the total future reward. AIXI does not require any previous knowledge of an environment, actions are chosen based on past perceptions, which are used to build a model of the environment. Let $\mathcal{M}$ be the model class of all chronological semi-computable semi-measures and $K(\rho)$ the Kolmogorov Complexity of $\rho$. Then AIXI chooses in cycle $k$ an action
\[ a_k = \arg \max_{a_k} \sum_{o_kr_k} \dots \max_{a_m} \sum_{o_m r_m} (r_k + \dots + r_m) \sum_{\rho \in \mathcal{M}}2^{-K(\rho)} \rho(o_1r_1\dots o_m r_m|a_1\dots a_m, \rho) \]


\bigskip

Unfortunately the AIXI model is incomputable. For all practical applications, the agent must be approximated. One approach in approximating AIXI is the MC-AIXI-CTW \cite{VNHS09} model. Here the expectimax search is solved by an Monte-Carlo approach. The UTC \cite{UCT} algorithm is used to balance exploration and exploitation. The class of environment models used in the implementation is a mixture of $d$-th order Markov Decision Process. Notably, Context Tree Weighting allows efficient linear time computation of this rather general class of models \cite{CTW}.

\bigskip

In comparison to AIXI, at cycle $k$ MC-AIXI-CTW selects an action
\[ a_k = \arg \max_{a_k} \sum_{o_kr_k} \dots \max_{a_m} \sum_{o_m r_m} (r_k + \dots + r_m) \sum_{M \in \mathcal{C}_D}2^{-\Gamma_D(M)} \rho(o_1r_1\dots o_m r_m|a_1\dots a_m, M) \]
Here $\mathcal{C}_D$ is the class of all prediction suffix trees of maximum depth $D$, and $\Gamma_D(M)$ is the description length of a context tree $M$.

\bigskip

In the following we present our implementation of the MC-AIXI-CTW model. In Section~\ref{usr} we explain how to use the program and specify different options. Section~\ref{results} describes the results of the model on several experimental environments.


\section{\label{usr}User Manual}

Our approximation of aixi is written in C++ and requires g++ for compilation.

\subsection{Setup}

\setlength\parindent{20pt}
\noindent Compile:\\
\indent \textit{cd aixi}\\
\indent \textit{make}

\bigskip

\noindent Run:\\
\indent \textit{./aixi file.conf [-{}-option1=value1 -{}-option2=value2 \dots]}
\setlength\parindent{0pt}

Include trained ctw data?? I think this is a good idea Johannes

\subsection{Configuration Options}

Options can be either specified in the configuration file or passed directly as -{}-option=value to the program. Several configuration files are available, each specifies a particular environment and a set of default options. 

\subsubsection*{Available Options}

\paragraph{-{}-environment=env} Specifies the environment. Available environments are
\begin{itemize}
\itemsep0pt
\renewcommand\labelitemi{--}
    \item biased\textunderscore rock\textunderscore paper\textunderscore scissor
    \item coinflip
    \item kuhn\textunderscore poker
    \item pacman
    \item tiger
\end{itemize}

\paragraph{-{}-ct-depth=M} Set the maximum depth of the context tree used for prediction to $M$.
%\paragraph{-{}-reward-bits=N} Number of bits used to encode a reward.
%\paragraph{-{}-observation-bits=N} Number of bits used to encode an observation.
\paragraph{-{}-agent-horizon=N} The number of percept/action pairs to look forward.
%\paragraph{-{}-agent-actions=N} The number of distinct actions an agent can perform.
\paragraph{-{}-mc-timelimit=N} The number $N$ of MC simulations per cycle.
\paragraph{-{}-write-ct=file} Write CTW to file before agent termination.
\paragraph{-{}-load-ct=file} Specifies a (trained) CTW for the agent to load at initialisation.
\paragraph{-{}-log=file}
\paragraph{-{}-terminate-age=N} The number $N$ of agent/environment interaction cycles.
\paragraph{-{}-exploration=P} Probability $0 \leq P \leq 1$ that a action is chosen randomly.
\paragraph{-{}-explore-decay=D} Decay $0 \leq D \leq 1$ of exploration constant. $P$ is multiplied by $D$ in each cycle.
\paragraph{-{}-intermediate-ct=[1|0]} If set to 0 , no intermediate context tree at time $t=2^k$ is written. Default is 1. (TODO: I actually would prefer default 0 but that's not so important :)  Johannes)
%\paragraph{-{}-}


\section{\label{results}Experimental Results}
\subsection{Experimental Setup}

The performance of our agent was evaluated on five sample environments. For each environment the agent was allowed 100000 cycles to learn a model. During the learning process an exploratory constant was used. The performance of the model after various amounts of experience was then evaluated by running the agent without exploration for 5000 cycles, and the average reward per cycle reported.

The parameters used for learning each environment are shown in Figure~\ref{tab:setup}. The experiments themselves were performed using a 3.47GHz CPU with 4GB of RAM.

\begin{figure}[H]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Domain & CTW depth & $m$ & $\epsilon$ & $\gamma$ & $\rho$UCT Simulations\\\hline
Biased Rock-Paper-Scissor & 32 & 4 & 0.999 & 0.99999 & 500\\
Coinflip & x & x & x & x & x00\\
Kuhn-Poker & 42 & 2 & 0.99 & 0.9999 & 500\\
Partial Observable Pacman & 96 & 4 & 0.9999 & 0.99999 & 500\\
Tiger & 96 & 5 & 0.99 & 0.9999 & 500\\\hline
\end{tabular}
\caption{\label{tab:setup}MC-AIXI-CTW model learning configuration}
\end{figure}

$m$, $\epsilon$, $\gamma$ and $\rho$UCT Simulations correspond to the options "agent-horizon", "exploration", "explore-decay" and "mc-timelimit" as described previoiusly.

\subsection{Results}

\setlength\parindent{20pt}
All environments except for Pacman have a known optimal policy and reward. Figure~\ref{plot:rewards} shows the average reward obtained by the agent in four environments, using a model with various cycles of experience. Generally the agent's performance improves with training, and approaches optimality. While the agent manages to improve average reward, our results are not quite as succesful as those given in the original paper \ref{VNHS09}. There are two main explanations for this.

Only 500 simulations were used during each cycle of model evaluation, while Veness et al. indicated that up to 25000 simulations are required for near optimal performance on some environements. Fewer simulations provided poorer estimates for the $\rho$UCT sampling process, and may occasionally lead to the selection of the incorrect action.

The most likely contributor to the difference in performance is due the implementation of model prediction. Veness et al. implement a factored CTW model, which allows for far greater predictive capabilities by using a chain of action-conditional prediction suffix trees. Each tree essentially deals with a single bit of an environment's percept space. In contrast, our implementation uses only a single tree.

While far simpler, a single tree allows only a single model for predicting percepts, and notably cannot distinguish between observation and reward bits. Consequently larger depth trees must be used in order to extract information from percepts, and the implementation generates an overall environment model which is more complex than necessary. Rather than predicting observations and rewards individually, no differentiation between the two is possible using a single CTW tree. Clearly then, such an agent cannot be expected to perform as well as an agent which models percepts separately.

Nevertheless, even with this rather limited environment model, our agent still manages to perform reasonably well in a variety of test domains.

\begin{figure}
\includegraphics[width=0.5\textwidth]{plots/tiger}
\includegraphics[width=0.5\textwidth]{plots/rps}
\includegraphics[width=0.5\textwidth]{plots/kpok}
\includegraphics[width=0.5\textwidth]{plots/pacman}
%\includegraphics[width=386px]{plots/pacman}
\caption{\label{plot:rewards}Average Reward per Cycle vs Experience}
\end{figure}

Performance on Pacman is significantly worse than on the other environments. While an optimal policy and reward for this domain are currently unknown, average rewards of approximately 2 are entirely possible.\\
TODO: visual inspection of agent playing pacman...\\
Learns to move.\\
Does not actively chase ghosts under effects of power pills\\
Never won a single game.

\setlength\parindent{0pt}

\subsection{Further Experiments}

\begin{figure}
\includegraphics[width=\textwidth]{plots/coinflip_tests1}
\includegraphics[width=\textwidth]{plots/rps_tiger_1}
%\includegraphics[width=\textwidth]{plots/coinflip_tests}
%\includegraphics[width=\textwidth]{plots/rps_tiger_trim}
\caption{\label{plot:cointests}Further experiments}
\end{figure}

\paragraph{0.2 Coin vs 0.8 Coin}
We run the coinflip environment with the following settings:
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Domain & CTW depth & $m$ & $\epsilon$ & $\rho$UCT Simulations & bias $p$\\\hline
Coinflip A & 16 & 2 & 0.999 & 100 & 0.8\\
Coinflip B & 16 & 2 & 0.999 & 100 & 0.2\\\hline
\end{tabular}
The values are chosen to give reasonable speed and accuracy. We run the following experiments:

\begin{enumerate}
 \setlength{\itemsep}{0cm}%
  \setlength{\parskip}{0cm}%
\item run agent on A with exploration=0.2 and save context tree\label{coinflip_a_ct}
\item run agent on A with exploration=0
\item load context tree generated in \ref{coinflip_a_ct}. and run agent on A with exploration=0
\item run agent on B, exploration = 0.2 (expected: same result as in 1.)
\item run agent on B, exploration = 0 (expected: same result as in 2.)
\item load tree 1, run agent on B with exploration = 0
\item load tree 1, run agent on B with exploration = 0.2
\end{enumerate}

\paragraph{Biased rps vs tiger}

\begin{tabular}{|l|c|c|c|c|c|}
\hline
Domain & CTW depth & $m$ & $\epsilon$ &$\gamma $ & $\rho$ UCT Simulations\\\hline
Tiger & 32 & 5 & 0.999 & 0.99 & 500\\
rps & 32 & 5 & 0.999 & 0.99 & 500\\\hline
\end{tabular}
\begin{enumerate}
 \setlength{\itemsep}{0cm}%
  \setlength{\parskip}{0cm}%
\item run agent in Tiger environment and save context tree
\item load context tree and run Tiger environment with exploration=0
\item run rps environment
\item run rps environment with exploration=0
\item load tiger context tree and run rps environment 
\item load tiger context tree and run rps environment with exploration=0
\end{enumerate}


%sample graph
%\begin{figure}
%\input{plots/coinflip.tex}
%\caption{\label{plot:coinflip}Results on the coinflip environment}
%\end{figure}


\subsection{Discussion}

Include statistics about cycles required for optimal performance, time per cycle as in the VNHS paper \cite{VNHS09}. Also note the number of simulations required at each cycle for near optimal performance.


\bibliographystyle{alpha}
\bibliography{references}

\end{document}
